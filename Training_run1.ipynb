{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "KKgPYoi79oxf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (3.1.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras) (0.0.7)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras) (3.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras) (1.24.3)\n",
      "Requirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras) (4.5.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting wandb\n",
      "  Downloading wandb-0.16.5-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.43.0-py2.py3-none-any.whl (264 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (65.6.3)\n",
      "Collecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Collecting Click!=8.0.0,>=7.1\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: appdirs, smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, Click, gitdb, GitPython, wandb\n",
      "Successfully installed Click-8.1.7 GitPython-3.1.42 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.11 protobuf-4.25.3 sentry-sdk-1.43.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install keras\n",
    "!pip install wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "Ltm8kDvSTQK-",
    "outputId": "8f610d79-0587-4f75-9853-10dd36cff851"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InitialCondition</th>\n",
       "      <th>Trajectory</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-2.0, -1.4471544715447155, -1.099526161982576...</td>\n",
       "      <td>([-2.010936428329985, -2.0217547169615546, -2....</td>\n",
       "      <td>Bounded orbit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(-2.0, -1.5447154471544715, -1.189881698919723...</td>\n",
       "      <td>([-2.0118383750574815, -2.023555345946939, -2....</td>\n",
       "      <td>Bounded orbit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(-2.0, -0.21138211382113803, -0.13653797119674...</td>\n",
       "      <td>([-2.001322141896985, -2.00255775461015, -2.00...</td>\n",
       "      <td>Bounded orbit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(-2.0, 0.5040650406504068, 0.3337940367874922,...</td>\n",
       "      <td>([-1.996616905942439, -1.993143643565462, -1.9...</td>\n",
       "      <td>Bounded orbit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(-2.0, -0.9268292682926829, -0.650604284584492...</td>\n",
       "      <td>([-2.0064555976142824, -2.012810042355882, -2....</td>\n",
       "      <td>Bounded orbit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>(-1.5121951219512195, 0.016260162601626327, 0....</td>\n",
       "      <td>([-1.5121251443240515, -1.512037597344692, -1....</td>\n",
       "      <td>Crash into primary 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>(-1.5121951219512195, -1.8048780487804879, -1....</td>\n",
       "      <td>([-1.5254269519443, -1.5385757534377793, -1.55...</td>\n",
       "      <td>Crash into primary 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>(-1.5121951219512195, 0.7317073170731709, 0.35...</td>\n",
       "      <td>([-1.508605601099163, -1.5049843456167733, -1....</td>\n",
       "      <td>Crash into primary 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>(-1.4796747967479673, -1.8373983739837398, -1....</td>\n",
       "      <td>([-1.4931627647983017, -1.506569316351737, -1....</td>\n",
       "      <td>Crash into primary 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>(-1.5121951219512195, 0.5040650406504068, 0.21...</td>\n",
       "      <td>([-1.509994904714787, -1.5077709416947869, -1....</td>\n",
       "      <td>Crash into primary 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       InitialCondition  \\\n",
       "0     (-2.0, -1.4471544715447155, -1.099526161982576...   \n",
       "1     (-2.0, -1.5447154471544715, -1.189881698919723...   \n",
       "2     (-2.0, -0.21138211382113803, -0.13653797119674...   \n",
       "3     (-2.0, 0.5040650406504068, 0.3337940367874922,...   \n",
       "4     (-2.0, -0.9268292682926829, -0.650604284584492...   \n",
       "...                                                 ...   \n",
       "1995  (-1.5121951219512195, 0.016260162601626327, 0....   \n",
       "1996  (-1.5121951219512195, -1.8048780487804879, -1....   \n",
       "1997  (-1.5121951219512195, 0.7317073170731709, 0.35...   \n",
       "1998  (-1.4796747967479673, -1.8373983739837398, -1....   \n",
       "1999  (-1.5121951219512195, 0.5040650406504068, 0.21...   \n",
       "\n",
       "                                             Trajectory        Classification  \n",
       "0     ([-2.010936428329985, -2.0217547169615546, -2....         Bounded orbit  \n",
       "1     ([-2.0118383750574815, -2.023555345946939, -2....         Bounded orbit  \n",
       "2     ([-2.001322141896985, -2.00255775461015, -2.00...         Bounded orbit  \n",
       "3     ([-1.996616905942439, -1.993143643565462, -1.9...         Bounded orbit  \n",
       "4     ([-2.0064555976142824, -2.012810042355882, -2....         Bounded orbit  \n",
       "...                                                 ...                   ...  \n",
       "1995  ([-1.5121251443240515, -1.512037597344692, -1....  Crash into primary 1  \n",
       "1996  ([-1.5254269519443, -1.5385757534377793, -1.55...  Crash into primary 1  \n",
       "1997  ([-1.508605601099163, -1.5049843456167733, -1....  Crash into primary 1  \n",
       "1998  ([-1.4931627647983017, -1.506569316351737, -1....  Crash into primary 1  \n",
       "1999  ([-1.509994904714787, -1.5077709416947869, -1....  Crash into primary 1  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('dataset1.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coordinates</th>\n",
       "      <th>Timesteps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Coordinates, Timesteps]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check on the shape of the dataset, to find out along which dimension I should pad\n",
    "trajectory_shapes = []\n",
    "\n",
    "# Iterate over each trajectory and get its shape\n",
    "for index, row in df.iterrows():\n",
    "    trajectory = np.array(row['Trajectory'])\n",
    "    trajectory_shapes.append(trajectory.shape)  # Append the shape to the list\n",
    "\n",
    "# Now you have a list of the shapes of each trajectory\n",
    "# You can convert this list to a DataFrame for easy inspection\n",
    "shapes_df = pd.DataFrame(trajectory_shapes, columns=['Coordinates', 'Timesteps'])\n",
    "filtered_df = shapes_df[shapes_df['Coordinates'] != 4]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_tensors = [torch.tensor(t, dtype=torch.float32) for t in trajectories]\n",
    "\n",
    "# Find the maximum number of features across all timesteps in all trajectories\n",
    "max_timesteps = max(t.shape[1] for t in trajectory_tensors)\n",
    "\n",
    "# Pad the trajectories along the features dimension and create masks\n",
    "padded_trajectories = []\n",
    "masks = []\n",
    "\n",
    "for t in trajectory_tensors:\n",
    "    # Calculate the padding needed for this trajectory\n",
    "    padding_needed = max_timesteps - t.shape[1]\n",
    "    \n",
    "    # Pad the trajectory and store it\n",
    "    padded = torch.nn.functional.pad(t, (0, padding_needed))\n",
    "    padded_trajectories.append(padded)\n",
    "    \n",
    "    # Create the mask for this trajectory: True for actual data, False for padding\n",
    "    mask = torch.ones_like(t, dtype=torch.bool)\n",
    "    mask = torch.nn.functional.pad(mask, (0, padding_needed), value=False)\n",
    "    masks.append(mask)\n",
    "\n",
    "# Stack the padded trajectories and masks into tensors\n",
    "padded_trajectories_tensor = torch.stack(padded_trajectories)\n",
    "masks_tensor = torch.stack(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 4, 9999])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_trajectories_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model architecture\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "def gaussian_init_(n_units, std=1):\n",
    "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
    "    Omega = sampler.sample((n_units, n_units))[..., 0]\n",
    "    return Omega\n",
    "\n",
    "\n",
    "class encoderNet(nn.Module):\n",
    "    def __init__(self, m, n, b, ALPHA = 1):\n",
    "        super(encoderNet, self).__init__()\n",
    "        self.N = m * n\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.N, 16*ALPHA)\n",
    "        self.fc2 = nn.Linear(16*ALPHA, 16*ALPHA)\n",
    "        self.fc3 = nn.Linear(16*ALPHA, b)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, self.N)\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class decoderNet(nn.Module):\n",
    "    def __init__(self, m, n, b, ALPHA = 1):\n",
    "        super(decoderNet, self).__init__()\n",
    "\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.b = b\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.fc1 = nn.Linear(b, 16*ALPHA)\n",
    "        self.fc2 = nn.Linear(16*ALPHA, 16*ALPHA)\n",
    "        self.fc3 = nn.Linear(16*ALPHA, m*n)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, self.b)\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        x = self.tanh(self.fc3(x))\n",
    "        x = x.view(-1, 1, self.m, self.n)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class dynamics(nn.Module):\n",
    "    def __init__(self, b, init_scale):\n",
    "        super(dynamics, self).__init__()\n",
    "        self.dynamics = nn.Linear(b, b, bias=False)\n",
    "        self.dynamics.weight.data = gaussian_init_(b, std=1)\n",
    "        U, _, V = torch.svd(self.dynamics.weight.data)\n",
    "        self.dynamics.weight.data = torch.mm(U, V.t()) * init_scale\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dynamics(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class dynamics_back(nn.Module):\n",
    "    def __init__(self, b, omega):\n",
    "        super(dynamics_back, self).__init__()\n",
    "        self.dynamics = nn.Linear(b, b, bias=False)\n",
    "        self.dynamics.weight.data = torch.pinverse(omega.dynamics.weight.data.t())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dynamics(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class koopmanAE(nn.Module):\n",
    "    def __init__(self, m, n, b, steps, steps_back, alpha = 1, init_scale=1):\n",
    "        super(koopmanAE, self).__init__()\n",
    "        self.steps = steps\n",
    "        self.steps_back = steps_back\n",
    "\n",
    "        self.encoder = encoderNet(m, n, b, ALPHA = alpha)\n",
    "        self.dynamics = dynamics(b, init_scale)\n",
    "        self.backdynamics = dynamics_back(b, self.dynamics)\n",
    "        self.decoder = decoderNet(m, n, b, ALPHA = alpha)\n",
    "\n",
    "\n",
    "    def forward(self, x, mode='forward'):\n",
    "        out = []\n",
    "        out_back = []\n",
    "        z = self.encoder(x.contiguous())\n",
    "        q = z.contiguous()\n",
    "\n",
    "\n",
    "        if mode == 'forward':\n",
    "            for _ in range(self.steps):\n",
    "                q = self.dynamics(q)\n",
    "                out.append(self.decoder(q))\n",
    "\n",
    "            out.append(self.decoder(z.contiguous()))\n",
    "            return out, out_back\n",
    "\n",
    "        if mode == 'backward':\n",
    "            for _ in range(self.steps_back):\n",
    "                q = self.backdynamics(q)\n",
    "                out_back.append(self.decoder(q))\n",
    "\n",
    "            out_back.append(self.decoder(z.contiguous()))\n",
    "            return out, out_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(padded_trajectories_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add noise to the dataset\n",
    "padded_trajectories_processed = padded_trajectories_tensor + np.random.standard_normal(padded_trajectories_tensor.shape)\n",
    "\n",
    "# Rotate to high-dimensional space for each trajectory separately\n",
    "num_trajectories = padded_trajectories_tensor.shape[0]\n",
    "\n",
    "# Rotate to high-dimensional space\n",
    "Q = np.random.standard_normal((64,4))\n",
    "Q,_ = np.linalg.qr(Q)\n",
    "\n",
    "processed_X = []    #processed X is a list of trajectories\n",
    "for i in range(num_trajectories):   \n",
    "    result = torch.matmul(padded_trajectories_processed[i].T, torch.tensor(Q.T))\n",
    "    min_val = torch.min(result)\n",
    "    max_val = torch.max(result)\n",
    "    result = 2 * (result - min_val) / (max_val - min_val) - 1  \n",
    "    processed_X.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9999, 64]) 2000\n"
     ]
    }
   ],
   "source": [
    "print(processed_X[0].shape, len(processed_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.autograd import grad\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    \"\"\"Set one seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get a gpu if available.\"\"\"\n",
    "    if torch.cuda.device_count()>0:\n",
    "        device = torch.device('cuda')\n",
    "        print(\"Connected to a GPU\")\n",
    "    else:\n",
    "        print(\"Using the CPU\")\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "\n",
    "def add_channels(X):\n",
    "    if len(X.shape) == 2:\n",
    "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
    "\n",
    "    elif len(X.shape) == 3:\n",
    "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
    "\n",
    "    else:\n",
    "        return \"dimenional error\"\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        if m.weight is not None:\n",
    "            init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the train method\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, lr, weight_decay,\n",
    "          lamb, num_epochs, learning_rate_change, epoch_update,\n",
    "          nu=0.0, eta=0.0, backward=0, steps=1, steps_back=1, gradclip=1):\n",
    "        # Initialize wandb project\n",
    "    wandb.init(project=\"Koopman Autoencoder\", name=\"training-run-name\")\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    wandb.config.lr = lr\n",
    "    wandb.config.weight_decay = weight_decay\n",
    "    wandb.config.lamb = lamb\n",
    "    wandb.config.num_epochs = num_epochs\n",
    "    wandb.config.learning_rate_change = learning_rate_change\n",
    "    wandb.config.epoch_update = epoch_update\n",
    "    wandb.config.nu = nu\n",
    "    wandb.config.eta = eta\n",
    "    wandb.config.backward = backward\n",
    "    wandb.config.steps = steps\n",
    "    wandb.config.steps_back = steps_back\n",
    "    wandb.config.gradclip = gradclip\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    device = get_device()\n",
    "\n",
    "\n",
    "    def lr_scheduler(optimizer, epoch, lr_decay_rate=0.8, decayEpoch=[]):\n",
    "                    \"\"\"Decay learning rate by a factor of lr_decay_rate every lr_decay_epoch epochs\"\"\"\n",
    "                    if epoch in decayEpoch:\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] *= lr_decay_rate\n",
    "                        return optimizer\n",
    "                    else:\n",
    "                        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "\n",
    "\n",
    "    epoch_hist = []\n",
    "    loss_hist = []\n",
    "    epoch_loss = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        #print(epoch)\n",
    "        for batch_idx, data_list in enumerate(train_loader):\n",
    "            #print(\"batch index\", batch_idx)\n",
    "            #print(\"data list total length\" , len(data_list))\n",
    "            #print(\"first item in data list length\", data_list[0].shape)\n",
    "            model.train()\n",
    "            out, out_back = model(data_list[0].to(device), mode='forward')\n",
    "\n",
    "\n",
    "\n",
    "            for k in range(steps):\n",
    "                if k == 0:\n",
    "                    loss_fwd = criterion(out[k], data_list[k+1].to(device))\n",
    "                    #print(\"loss between\", out[k].shape, data_list[k+1].shape)\n",
    "                else:\n",
    "                    loss_fwd += criterion(out[k], data_list[k+1].to(device))\n",
    "\n",
    "\n",
    "            loss_identity = criterion(out[-1], data_list[0].to(device)) * steps\n",
    "\n",
    "\n",
    "            loss_bwd = 0.0\n",
    "            loss_consist = 0.0\n",
    "\n",
    "            loss_bwd = 0.0\n",
    "            loss_consist = 0.0\n",
    "\n",
    "            if backward == 1:\n",
    "                out, out_back = model(data_list[-1].to(device), mode='backward')\n",
    "\n",
    "\n",
    "                for k in range(steps_back):\n",
    "\n",
    "                    if k == 0:\n",
    "                        loss_bwd = criterion(out_back[k], data_list[::-1][k+1].to(device))\n",
    "                    else:\n",
    "                        loss_bwd += criterion(out_back[k], data_list[::-1][k+1].to(device))\n",
    "\n",
    "\n",
    "                A = model.dynamics.dynamics.weight\n",
    "                B = model.backdynamics.dynamics.weight\n",
    "\n",
    "                K = A.shape[-1]\n",
    "\n",
    "                for k in range(1,K+1):\n",
    "                    As1 = A[:,:k]\n",
    "                    Bs1 = B[:k,:]\n",
    "                    As2 = A[:k,:]\n",
    "                    Bs2 = B[:,:k]\n",
    "\n",
    "                    Ik = torch.eye(k).float().to(device)\n",
    "\n",
    "                    if k == 1:\n",
    "                        loss_consist = (torch.sum((torch.mm(Bs1, As1) - Ik)**2) + \\\n",
    "                                         torch.sum((torch.mm(As2, Bs2) - Ik)**2) ) / (2.0*k)\n",
    "                    else:\n",
    "                        loss_consist += (torch.sum((torch.mm(Bs1, As1) - Ik)**2) + \\\n",
    "                                         torch.sum((torch.mm(As2, Bs2)-  Ik)**2) ) / (2.0*k)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                Ik = torch.eye(K).float().to(device)\n",
    "#                loss_consist = (torch.sum( (torch.mm(A, B)-Ik )**2)**1 + \\\n",
    "#                                         torch.sum( (torch.mm(B, A)-Ik)**2)**1 )\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "            loss = loss_fwd + lamb * loss_identity +  nu * loss_bwd + eta * loss_consist\n",
    "\n",
    "            # ===================backward====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), gradclip) # gradient clip\n",
    "            optimizer.step()\n",
    "\n",
    "        # schedule learning rate decay\n",
    "        # Log metrics\n",
    "        wandb.log({\"epoch\": epoch, \"loss\": loss.item(), \"loss_identity\": loss_identity.item(), \"loss_forward\": loss_fwd.item()})\n",
    "        lr_scheduler(optimizer, epoch, lr_decay_rate=learning_rate_change, decayEpoch=epoch_update)\n",
    "        loss_hist.append(loss)\n",
    "        epoch_loss.append(epoch)\n",
    "\n",
    "\n",
    "        if (epoch) % 20 == 0:\n",
    "                print('********** Epoche %s **********' %(epoch+1))\n",
    "\n",
    "                print(\"loss identity: \", loss_identity.item())\n",
    "                if backward == 1:\n",
    "                    print(\"loss backward: \", loss_bwd.item())\n",
    "                    print(\"loss consistent: \", loss_consist.item())\n",
    "                print(\"loss forward: \", loss_fwd.item())\n",
    "                print(\"loss sum: \", loss.item())\n",
    "\n",
    "\n",
    "                epoch_hist.append(epoch+1)\n",
    "\n",
    "                if hasattr(model.dynamics, 'dynamics'):\n",
    "                    w, _ = np.linalg.eig(model.dynamics.dynamics.weight.data.cpu().numpy())\n",
    "                    print(np.abs(w))\n",
    "\n",
    "\n",
    "    if backward == 1:\n",
    "        loss_consist = loss_consist.item()\n",
    "\n",
    "\n",
    "    return model, optimizer, [epoch_hist, loss_fwd.item(), loss_consist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "X train length torch.Size([9999, 64])\n",
      "X test length torch.Size([3999, 64])\n",
      "Training on trajectory 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:k2g4wjcp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training-run-name</strong> at: <a href='https://wandb.ai/narmal/Koopman%20Autoencoder/runs/k2g4wjcp/workspace' target=\"_blank\">https://wandb.ai/narmal/Koopman%20Autoencoder/runs/k2g4wjcp/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240326_202505-k2g4wjcp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:k2g4wjcp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbcc55f3f814ec3a0411f853954a7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111248042434454, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240326_202848-z6nvqton</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/narmal/Koopman%20Autoencoder/runs/z6nvqton/workspace' target=\"_blank\">training-run-name</a></strong> to <a href='https://wandb.ai/narmal/Koopman%20Autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/narmal/Koopman%20Autoencoder' target=\"_blank\">https://wandb.ai/narmal/Koopman%20Autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/narmal/Koopman%20Autoencoder/runs/z6nvqton/workspace' target=\"_blank\">https://wandb.ai/narmal/Koopman%20Autoencoder/runs/z6nvqton/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to a GPU\n",
      "********** Epoche 1 **********\n",
      "loss identity:  0.03639815375208855\n",
      "loss forward:  1.624560832977295\n",
      "loss sum:  1.660959005355835\n",
      "[0.83404857 1.0106785  0.71270967 0.71270967 0.6719378  0.6719378 ]\n",
      "********** Epoche 21 **********\n",
      "loss identity:  0.021318979561328888\n",
      "loss forward:  1.330122947692871\n",
      "loss sum:  1.3514419794082642\n",
      "[0.6784041  0.43134025 0.43134025 0.9992048  0.99262154 0.01942833]\n",
      "********** Epoche 41 **********\n",
      "loss identity:  0.03121667169034481\n",
      "loss forward:  1.2264845371246338\n",
      "loss sum:  1.2577011585235596\n",
      "[0.99973506 0.99973506 0.5042528  0.2536619  0.2536619  0.01592734]\n",
      "********** Epoche 61 **********\n",
      "loss identity:  0.032935380935668945\n",
      "loss forward:  1.3283480405807495\n",
      "loss sum:  1.3612834215164185\n",
      "[0.99220026 0.99220026 0.3642573  0.23483764 0.03640407 0.09255002]\n",
      "********** Epoche 81 **********\n",
      "loss identity:  0.023897280916571617\n",
      "loss forward:  1.3087165355682373\n",
      "loss sum:  1.3326138257980347\n",
      "[1.0005447  1.0005447  0.31555015 0.31555015 0.1322132  0.09658268]\n",
      "********** Epoche 101 **********\n",
      "loss identity:  0.04608709365129471\n",
      "loss forward:  1.2133787870407104\n",
      "loss sum:  1.2594659328460693\n",
      "[1.0008183  1.0008183  0.31237477 0.31237477 0.10505069 0.06566149]\n",
      "********** Epoche 121 **********\n",
      "loss identity:  0.016509221866726875\n",
      "loss forward:  1.239634394645691\n",
      "loss sum:  1.256143569946289\n",
      "[1.0008126  1.0008126  0.31033078 0.31033078 0.03928674 0.04965669]\n",
      "********** Epoche 141 **********\n",
      "loss identity:  0.02916841208934784\n",
      "loss forward:  1.3028000593185425\n",
      "loss sum:  1.3319684267044067\n",
      "[0.9951106  0.9951106  0.30380034 0.30380034 0.08963782 0.12575772]\n",
      "********** Epoche 161 **********\n",
      "loss identity:  0.0201503224670887\n",
      "loss forward:  1.248211145401001\n",
      "loss sum:  1.2683614492416382\n",
      "[0.9966564  0.9966564  0.29222104 0.29222104 0.10935938 0.06469227]\n",
      "********** Epoche 181 **********\n",
      "loss identity:  0.0136000020429492\n",
      "loss forward:  1.2690017223358154\n",
      "loss sum:  1.2826017141342163\n",
      "[0.99798816 0.99798816 0.29107618 0.29107618 0.0747652  0.06533539]\n",
      "********** Epoche 201 **********\n",
      "loss identity:  0.032030753791332245\n",
      "loss forward:  1.3549625873565674\n",
      "loss sum:  1.3869932889938354\n",
      "[0.9968323  0.9968323  0.2800205  0.2800205  0.02235568 0.10123976]\n",
      "********** Epoche 221 **********\n",
      "loss identity:  0.0147080197930336\n",
      "loss forward:  1.2964425086975098\n",
      "loss sum:  1.3111505508422852\n",
      "[0.99932283 0.99932283 0.13784789 0.13784789 0.04041404 0.07052923]\n",
      "********** Epoche 241 **********\n",
      "loss identity:  0.0438816100358963\n",
      "loss forward:  1.1716315746307373\n",
      "loss sum:  1.2155132293701172\n",
      "[0.9986584  0.9986584  0.16645367 0.16645367 0.11402297 0.09110256]\n",
      "********** Epoche 261 **********\n",
      "loss identity:  0.043663349002599716\n",
      "loss forward:  1.2879221439361572\n",
      "loss sum:  1.3315855264663696\n",
      "[1.0000838  1.0000838  0.16787253 0.16787253 0.10662521 0.10662521]\n",
      "********** Epoche 281 **********\n",
      "loss identity:  0.028496159240603447\n",
      "loss forward:  1.3547552824020386\n",
      "loss sum:  1.383251428604126\n",
      "[0.9989623  0.9989623  0.17476541 0.00331352 0.09002158 0.09002158]\n",
      "********** Epoche 301 **********\n",
      "loss identity:  0.030798308551311493\n",
      "loss forward:  1.1468685865402222\n",
      "loss sum:  1.1776669025421143\n",
      "[0.99821615 0.99821615 0.1784117  0.1784117  0.06392281 0.11714087]\n",
      "********** Epoche 321 **********\n",
      "loss identity:  0.019786078482866287\n",
      "loss forward:  1.270080804824829\n",
      "loss sum:  1.2898669242858887\n",
      "[0.17531835 0.0964056  0.042808   0.042808   0.99762267 0.99762267]\n",
      "********** Epoche 341 **********\n",
      "loss identity:  0.03800715133547783\n",
      "loss forward:  1.2422457933425903\n",
      "loss sum:  1.2802529335021973\n",
      "[0.9993563  0.9993563  0.1750491  0.05690452 0.05690452 0.07958646]\n",
      "********** Epoche 361 **********\n",
      "loss identity:  0.06467578560113907\n",
      "loss forward:  1.3498951196670532\n",
      "loss sum:  1.414570927619934\n",
      "[0.99924886 0.99924886 0.04291096 0.04291096 0.0834906  0.1769267 ]\n",
      "********** Epoche 381 **********\n",
      "loss identity:  0.016960669308900833\n",
      "loss forward:  1.2917033433914185\n",
      "loss sum:  1.3086639642715454\n",
      "[0.0050854  0.08475248 0.08475248 0.20068373 0.997658   0.997658  ]\n",
      "********** Epoche 401 **********\n",
      "loss identity:  0.013456943444907665\n",
      "loss forward:  1.3051775693893433\n",
      "loss sum:  1.3186345100402832\n",
      "[0.20820972 0.20820972 0.08594917 0.12803178 0.9957772  0.9957772 ]\n",
      "********** Epoche 421 **********\n",
      "loss identity:  0.018108434975147247\n",
      "loss forward:  1.2753485441207886\n",
      "loss sum:  1.29345703125\n",
      "[0.9985793  0.9985793  0.05237226 0.05237226 0.06610441 0.06610441]\n",
      "********** Epoche 441 **********\n",
      "loss identity:  0.03602702543139458\n",
      "loss forward:  1.2489901781082153\n",
      "loss sum:  1.2850172519683838\n",
      "[1.0001725  1.0001725  0.07266721 0.04277987 0.04277987 0.01631972]\n",
      "********** Epoche 461 **********\n",
      "loss identity:  0.027203906327486038\n",
      "loss forward:  1.3101345300674438\n",
      "loss sum:  1.3373384475708008\n",
      "[0.99772066 0.99772066 0.10856462 0.09044421 0.09044421 0.02504474]\n",
      "********** Epoche 481 **********\n",
      "loss identity:  0.017608266323804855\n",
      "loss forward:  1.3183852434158325\n",
      "loss sum:  1.3359935283660889\n",
      "[0.9990232  0.9990232  0.00881484 0.09592918 0.05441201 0.038555  ]\n",
      "********** Epoche 501 **********\n",
      "loss identity:  0.02169729210436344\n",
      "loss forward:  1.3787603378295898\n",
      "loss sum:  1.4004576206207275\n",
      "[0.9948623  0.9948623  0.15505823 0.10607373 0.04684138 0.01366492]\n",
      "********** Epoche 521 **********\n",
      "loss identity:  0.04074373096227646\n",
      "loss forward:  1.225054144859314\n",
      "loss sum:  1.2657978534698486\n",
      "[1.0008817  1.0008817  0.21352725 0.08799812 0.08799812 0.01287913]\n",
      "********** Epoche 541 **********\n",
      "loss identity:  0.021527698263525963\n",
      "loss forward:  1.269705891609192\n",
      "loss sum:  1.2912335395812988\n",
      "[0.99841684 0.99841684 0.08992956 0.08992956 0.04548748 0.04548748]\n",
      "********** Epoche 561 **********\n",
      "loss identity:  0.026992233470082283\n",
      "loss forward:  1.2229160070419312\n",
      "loss sum:  1.249908208847046\n",
      "[0.99865323 0.99865323 0.12119795 0.12119795 0.05399421 0.07924388]\n",
      "********** Epoche 581 **********\n",
      "loss identity:  0.0303378626704216\n",
      "loss forward:  1.2991679906845093\n",
      "loss sum:  1.3295058012008667\n",
      "[1.0010084  1.0010084  0.18012969 0.05378848 0.05378848 0.04102533]\n",
      "Training on trajectory 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:z6nvqton) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▃▄▄▂▃▅▃▂▃▁▄▄▂▃▄▃▄▃▃▂▂▄▃▂▃▂▃▄▁▃▂▂▃▄▄▁▄▃▂</td></tr><tr><td>loss_forward</td><td>█▃▄▄▂▄▅▃▂▃▁▄▄▂▃▄▃▄▃▃▃▂▄▃▂▃▂▃▄▁▃▂▂▃▄▄▁▄▃▂</td></tr><tr><td>loss_identity</td><td>▅▄▃▃▂▃▃▃▅▄▃▄▄▄█▃▅▃▄▃▂▃▂▃▄▄▂▂▆▄▃▇▁▂▃▄▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>599</td></tr><tr><td>loss</td><td>1.25465</td></tr><tr><td>loss_forward</td><td>1.23791</td></tr><tr><td>loss_identity</td><td>0.01674</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training-run-name</strong> at: <a href='https://wandb.ai/narmal/Koopman%20Autoencoder/runs/z6nvqton/workspace' target=\"_blank\">https://wandb.ai/narmal/Koopman%20Autoencoder/runs/z6nvqton/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240326_202848-z6nvqton/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:z6nvqton). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240326_232036-3o3xg807</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/narmal/Koopman%20Autoencoder/runs/3o3xg807/workspace' target=\"_blank\">training-run-name</a></strong> to <a href='https://wandb.ai/narmal/Koopman%20Autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/narmal/Koopman%20Autoencoder' target=\"_blank\">https://wandb.ai/narmal/Koopman%20Autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/narmal/Koopman%20Autoencoder/runs/3o3xg807/workspace' target=\"_blank\">https://wandb.ai/narmal/Koopman%20Autoencoder/runs/3o3xg807/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to a GPU\n",
      "********** Epoche 1 **********\n",
      "loss identity:  0.0597509890794754\n",
      "loss forward:  1.779867172241211\n",
      "loss sum:  1.83961820602417\n",
      "[0.9957807  0.9957807  0.10460124 0.05039342 0.05039342 0.03789065]\n",
      "********** Epoche 21 **********\n",
      "loss identity:  0.0616505965590477\n",
      "loss forward:  1.753999948501587\n",
      "loss sum:  1.8156505823135376\n",
      "[1.004217   1.004217   0.01733005 0.01733005 0.07294662 0.07294662]\n",
      "********** Epoche 41 **********\n",
      "loss identity:  0.025854449719190598\n",
      "loss forward:  1.7250511646270752\n",
      "loss sum:  1.7509056329727173\n",
      "[1.000318   1.000318   0.08865938 0.08865938 0.01816951 0.04401044]\n",
      "********** Epoche 61 **********\n",
      "loss identity:  0.06471572816371918\n",
      "loss forward:  1.7382997274398804\n",
      "loss sum:  1.8030154705047607\n",
      "[1.002128   1.002128   0.02414503 0.12722726 0.12722726 0.11845732]\n",
      "********** Epoche 81 **********\n",
      "loss identity:  0.05041515827178955\n",
      "loss forward:  1.8288872241973877\n",
      "loss sum:  1.8793023824691772\n",
      "[0.99945444 0.99945444 0.1093993  0.02409879 0.04046874 0.04046874]\n",
      "********** Epoche 101 **********\n",
      "loss identity:  0.05207888409495354\n",
      "loss forward:  1.7656768560409546\n",
      "loss sum:  1.8177556991577148\n",
      "[1.0012479  1.0012479  0.17605346 0.07445853 0.03478323 0.04765536]\n",
      "********** Epoche 121 **********\n",
      "loss identity:  0.025592267513275146\n",
      "loss forward:  1.6573386192321777\n",
      "loss sum:  1.6829309463500977\n",
      "[1.0025504  1.0025504  0.16704497 0.1321352  0.06243326 0.0325403 ]\n",
      "********** Epoche 141 **********\n",
      "loss identity:  0.06460674852132797\n",
      "loss forward:  1.6941841840744019\n",
      "loss sum:  1.7587909698486328\n",
      "[0.997095   0.997095   0.20197645 0.03524397 0.03524397 0.10486526]\n",
      "********** Epoche 161 **********\n",
      "loss identity:  0.021566947922110558\n",
      "loss forward:  1.7988864183425903\n",
      "loss sum:  1.820453405380249\n",
      "[0.9992041  0.9992041  0.11237763 0.11237763 0.03774072 0.06376477]\n",
      "********** Epoche 181 **********\n",
      "loss identity:  0.03222868591547012\n",
      "loss forward:  1.6932114362716675\n",
      "loss sum:  1.7254401445388794\n",
      "[0.99992305 0.99992305 0.21373649 0.05402056 0.05402056 0.06258199]\n",
      "********** Epoche 201 **********\n",
      "loss identity:  0.03929079696536064\n",
      "loss forward:  1.680375337600708\n",
      "loss sum:  1.7196661233901978\n",
      "[0.99768674 0.99768674 0.07417188 0.07417188 0.00833802 0.02799882]\n",
      "********** Epoche 221 **********\n",
      "loss identity:  0.08759651333093643\n",
      "loss forward:  1.6584084033966064\n",
      "loss sum:  1.7460049390792847\n",
      "[0.999553   0.999553   0.21834789 0.11377786 0.03513597 0.03513597]\n",
      "********** Epoche 241 **********\n",
      "loss identity:  0.031389329582452774\n",
      "loss forward:  1.6818053722381592\n",
      "loss sum:  1.713194727897644\n",
      "[1.0001227  1.0001227  0.12965207 0.07816607 0.07816607 0.03079114]\n",
      "********** Epoche 261 **********\n",
      "loss identity:  0.06121882051229477\n",
      "loss forward:  1.817792296409607\n",
      "loss sum:  1.8790111541748047\n",
      "[0.19583286 0.11165162 0.04193445 0.00931211 0.9965637  0.9965637 ]\n",
      "********** Epoche 281 **********\n",
      "loss identity:  0.0631122887134552\n",
      "loss forward:  1.8818961381912231\n",
      "loss sum:  1.945008397102356\n",
      "[0.9977142  0.9977142  0.00330461 0.07845729 0.07845729 0.08264324]\n",
      "********** Epoche 301 **********\n",
      "loss identity:  0.02910649962723255\n",
      "loss forward:  1.706373691558838\n",
      "loss sum:  1.7354801893234253\n",
      "[0.99834365 0.99834365 0.11026415 0.02400161 0.04990202 0.04990202]\n",
      "********** Epoche 321 **********\n",
      "loss identity:  0.03155219182372093\n",
      "loss forward:  1.7222929000854492\n",
      "loss sum:  1.7538450956344604\n",
      "[1.0008838  1.0008838  0.10084771 0.05576226 0.01898743 0.00636819]\n",
      "********** Epoche 341 **********\n",
      "loss identity:  0.02472943440079689\n",
      "loss forward:  1.7859468460083008\n",
      "loss sum:  1.8106763362884521\n",
      "[0.99578387 0.99578387 0.1450099  0.06217803 0.06217803 0.06169454]\n",
      "********** Epoche 361 **********\n",
      "loss identity:  0.0446699894964695\n",
      "loss forward:  1.723669171333313\n",
      "loss sum:  1.7683391571044922\n",
      "[0.9950164  0.9950164  0.15876603 0.15876603 0.03006697 0.03006697]\n",
      "********** Epoche 381 **********\n",
      "loss identity:  0.04235685244202614\n",
      "loss forward:  1.75890052318573\n",
      "loss sum:  1.8012573719024658\n",
      "[1.0001396  1.0001396  0.05229954 0.05229954 0.04812848 0.03905671]\n",
      "********** Epoche 401 **********\n",
      "loss identity:  0.03150063753128052\n",
      "loss forward:  1.7447803020477295\n",
      "loss sum:  1.7762808799743652\n",
      "[0.99798435 1.0034525  0.11507454 0.11507454 0.02917977 0.04783832]\n",
      "********** Epoche 421 **********\n",
      "loss identity:  0.032804422080516815\n",
      "loss forward:  1.6050074100494385\n",
      "loss sum:  1.6378117799758911\n",
      "[1.0019356e+00 1.0019356e+00 1.7647253e-04 9.1410570e-02 9.1410570e-02\n",
      " 7.9790846e-02]\n",
      "********** Epoche 441 **********\n",
      "loss identity:  0.03607913479208946\n",
      "loss forward:  1.7055675983428955\n",
      "loss sum:  1.7416467666625977\n",
      "[1.0005594  1.0005594  0.08587956 0.04287633 0.04287633 0.023218  ]\n",
      "********** Epoche 461 **********\n",
      "loss identity:  0.028454113751649857\n",
      "loss forward:  1.7463101148605347\n",
      "loss sum:  1.7747641801834106\n",
      "[0.9996897  0.9996897  0.12502448 0.06580733 0.06580733 0.04347508]\n",
      "********** Epoche 481 **********\n",
      "loss identity:  0.06489858031272888\n",
      "loss forward:  1.6866143941879272\n",
      "loss sum:  1.7515130043029785\n",
      "[1.002134   1.002134   0.08436124 0.04614545 0.04614545 0.01960822]\n",
      "********** Epoche 501 **********\n",
      "loss identity:  0.022588014602661133\n",
      "loss forward:  1.8549479246139526\n",
      "loss sum:  1.8775359392166138\n",
      "[0.9959541  0.9959541  0.08533762 0.08533762 0.04173576 0.11550107]\n",
      "********** Epoche 521 **********\n",
      "loss identity:  0.04794594645500183\n",
      "loss forward:  1.8585256338119507\n",
      "loss sum:  1.906471610069275\n",
      "[0.9994347  0.9994347  0.15744056 0.00763132 0.05731194 0.05731194]\n",
      "********** Epoche 541 **********\n",
      "loss identity:  0.025464827194809914\n",
      "loss forward:  1.6399190425872803\n",
      "loss sum:  1.6653838157653809\n",
      "[0.996323   0.996323   0.17943752 0.08089052 0.08089052 0.05677176]\n",
      "********** Epoche 561 **********\n",
      "loss identity:  0.0607324056327343\n",
      "loss forward:  1.7643043994903564\n",
      "loss sum:  1.8250367641448975\n",
      "[1.0002565  1.0002565  0.11294419 0.01059831 0.03172539 0.04185978]\n",
      "********** Epoche 581 **********\n",
      "loss identity:  0.06585092097520828\n",
      "loss forward:  1.7403169870376587\n",
      "loss sum:  1.8061679601669312\n",
      "[1.0010356  1.0010356  0.11099976 0.0510551  0.0510551  0.05486063]\n",
      "Training on trajectory 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3o3xg807) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▅▇▇▃▅▅▄▄▁▆▅▅▇▄▆▅▃▅█▄▄▅▅▄▆▂▇▃▄▄▄▇▁▆▆▄▆█▅▆</td></tr><tr><td>loss_forward</td><td>▄▇▆▃▅▅▄▄▁▆▅▅▇▄▅▆▃▅▇▄▄▄▄▄▅▂▆▃▄▄▃▇▁▆▆▃▆█▅▆</td></tr><tr><td>loss_identity</td><td>▄▃▅▄▃▅▅▃▃▂▃▄▆▄█▂▂▂▄▃▄▃▄▄▅▂▄▁▁▂▄▃▂▄▄▄▁▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>599</td></tr><tr><td>loss</td><td>1.83614</td></tr><tr><td>loss_forward</td><td>1.80501</td></tr><tr><td>loss_identity</td><td>0.03114</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training-run-name</strong> at: <a href='https://wandb.ai/narmal/Koopman%20Autoencoder/runs/3o3xg807/workspace' target=\"_blank\">https://wandb.ai/narmal/Koopman%20Autoencoder/runs/3o3xg807/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240326_232036-3o3xg807/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3o3xg807). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240327_021140-4yi0wc30</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/narmal/Koopman%20Autoencoder/runs/4yi0wc30/workspace' target=\"_blank\">training-run-name</a></strong> to <a href='https://wandb.ai/narmal/Koopman%20Autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/narmal/Koopman%20Autoencoder' target=\"_blank\">https://wandb.ai/narmal/Koopman%20Autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/narmal/Koopman%20Autoencoder/runs/4yi0wc30/workspace' target=\"_blank\">https://wandb.ai/narmal/Koopman%20Autoencoder/runs/4yi0wc30/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to a GPU\n",
      "********** Epoche 1 **********\n",
      "loss identity:  0.03199414163827896\n",
      "loss forward:  1.64121413230896\n",
      "loss sum:  1.673208236694336\n",
      "[1.0001674  1.0001674  0.07416733 0.03040887 0.03040887 0.04095354]\n",
      "********** Epoche 21 **********\n",
      "loss identity:  0.02037563920021057\n",
      "loss forward:  1.7225559949874878\n",
      "loss sum:  1.742931604385376\n",
      "[1.0008655  1.0008655  0.04386467 0.08282441 0.08282441 0.08438905]\n",
      "********** Epoche 41 **********\n",
      "loss identity:  0.023433193564414978\n",
      "loss forward:  1.7425323724746704\n",
      "loss sum:  1.7659655809402466\n",
      "[0.997307   0.997307   0.20895213 0.20895213 0.0638601  0.12756237]\n",
      "********** Epoche 61 **********\n",
      "loss identity:  0.020021842792630196\n",
      "loss forward:  1.738541841506958\n",
      "loss sum:  1.7585636377334595\n",
      "[0.9978714  0.9978714  0.02209388 0.05131868 0.03805735 0.03805735]\n",
      "********** Epoche 81 **********\n",
      "loss identity:  0.02216717042028904\n",
      "loss forward:  1.626802921295166\n",
      "loss sum:  1.648970127105713\n",
      "[0.99996495 0.99996495 0.10656064 0.02459806 0.06155613 0.03478329]\n",
      "********** Epoche 101 **********\n",
      "loss identity:  0.031125033274292946\n",
      "loss forward:  1.714003324508667\n",
      "loss sum:  1.7451283931732178\n",
      "[0.00996778 0.02555213 0.11061407 0.15818167 0.9959545  0.9959545 ]\n",
      "********** Epoche 121 **********\n",
      "loss identity:  0.020207194611430168\n",
      "loss forward:  1.6180100440979004\n",
      "loss sum:  1.6382172107696533\n",
      "[0.99991304 0.99991304 0.11383054 0.08880265 0.01476527 0.01476527]\n",
      "********** Epoche 141 **********\n",
      "loss identity:  0.03757685050368309\n",
      "loss forward:  1.5915052890777588\n",
      "loss sum:  1.6290820837020874\n",
      "[1.0011681  1.0011681  0.08439246 0.04966401 0.01961479 0.00913742]\n",
      "********** Epoche 161 **********\n",
      "loss identity:  0.02565750852227211\n",
      "loss forward:  1.6767117977142334\n",
      "loss sum:  1.7023693323135376\n",
      "[0.9950718  0.9950718  0.00998162 0.14046882 0.04962448 0.07753943]\n",
      "********** Epoche 181 **********\n",
      "loss identity:  0.030478686094284058\n",
      "loss forward:  1.564286470413208\n",
      "loss sum:  1.5947651863098145\n",
      "[1.0017701  1.0017701  0.15187974 0.08951173 0.08951173 0.02891254]\n",
      "********** Epoche 201 **********\n",
      "loss identity:  0.01791784167289734\n",
      "loss forward:  1.7502142190933228\n",
      "loss sum:  1.7681320905685425\n",
      "[1.0047286  1.0047286  0.02195154 0.01619273 0.05824868 0.05824868]\n",
      "********** Epoche 221 **********\n",
      "loss identity:  0.024677004665136337\n",
      "loss forward:  1.5979998111724854\n",
      "loss sum:  1.6226768493652344\n",
      "[0.99777055 0.99777055 0.1319284  0.04178515 0.04178515 0.03647387]\n",
      "********** Epoche 241 **********\n",
      "loss identity:  0.021861054003238678\n",
      "loss forward:  1.6318219900131226\n",
      "loss sum:  1.653683066368103\n",
      "[0.99821293 0.99821293 0.00583436 0.07756935 0.05581068 0.05581068]\n",
      "********** Epoche 261 **********\n",
      "loss identity:  0.029142899438738823\n",
      "loss forward:  1.6075611114501953\n",
      "loss sum:  1.6367039680480957\n",
      "[1.002105   1.002105   0.03369039 0.03369039 0.09293553 0.09293553]\n",
      "********** Epoche 281 **********\n",
      "loss identity:  0.021394262090325356\n",
      "loss forward:  1.7117429971694946\n",
      "loss sum:  1.7331372499465942\n",
      "[1.0002878  1.0002878  0.00231598 0.06731386 0.04244899 0.04244899]\n",
      "********** Epoche 301 **********\n",
      "loss identity:  0.02654707431793213\n",
      "loss forward:  1.6722238063812256\n",
      "loss sum:  1.6987708806991577\n",
      "[1.0004371  1.0004371  0.15425055 0.10241975 0.03644839 0.0438089 ]\n",
      "********** Epoche 321 **********\n",
      "loss identity:  0.04628409445285797\n",
      "loss forward:  1.6726101636886597\n",
      "loss sum:  1.7188942432403564\n",
      "[0.9965857  0.9965857  0.02223773 0.06801275 0.06801275 0.08204839]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "########################################### Model Hyperparameters ##############################################\n",
    "\n",
    "#Declaring some hyperparameters - default values\n",
    "lamb = 1\n",
    "steps = 100\n",
    "lr =1e-2\n",
    "bottleneck= 6\n",
    "lr_update = [30, 200, 400, 500],\n",
    "epochs = 600\n",
    "batch = 64\n",
    "steps_back = 8\n",
    "wd = 0\n",
    "lr_decay = 0.2\n",
    "nu =1e-1\n",
    "eta = 1e-2\n",
    "backward = 0\n",
    "gradclip = 0.05\n",
    "pred_steps = 1000\n",
    "m = 64\n",
    "n= 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################   TRAIN LOADERS ###########################################################\n",
    "train_loaders = []\n",
    "test_dataset_trajectories = []\n",
    "#should be len(processed_X)\n",
    "for i in range(100):\n",
    "    Xtrain = processed_X[i][0:9999]\n",
    "    Xtest = processed_X[i][6000:10000]\n",
    "    print(\"X train length\", Xtrain.shape)\n",
    "    print(\"X test length\", Xtest.shape)\n",
    "    #test_dataset_trajectories.append(Xtest)\n",
    "    Xtrain = add_channels(Xtrain)\n",
    "    Xtest = add_channels(Xtest)\n",
    "    Xtrain = Xtrain.float().contiguous()\n",
    "    Xtest = Xtest.float().contiguous()\n",
    "    trainDat = []\n",
    "    start = 0\n",
    "    for i in np.arange(steps,-1, -1):\n",
    "        if i == 0:\n",
    "            trainDat.append(Xtrain[start:].float())\n",
    "        else:\n",
    "            trainDat.append(Xtrain[start:-i].float())\n",
    "        start += 1\n",
    "\n",
    "    train_data = torch.utils.data.TensorDataset(*trainDat)\n",
    "    del(trainDat)\n",
    "\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(dataset = train_data,\n",
    "                              batch_size = batch,\n",
    "                              shuffle = True)\n",
    "\n",
    "    train_loaders.append(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "# Model\n",
    "#==============================================================================\n",
    "\n",
    "model = koopmanAE(m, n, bottleneck, steps, steps_back)\n",
    "\n",
    "\n",
    "############################ Printing the Model Architecture  #############################################\n",
    "#print('koopmanAE')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model = torch.nn.DataParallel(model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "# Model summary\n",
    "#==============================================================================\n",
    "'''\n",
    "print('**** Setup ****')\n",
    "print('Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
    "print('************')\n",
    "print(model)\n",
    "'''\n",
    "\n",
    "#==============================================================================\n",
    "# Start training - training separately on each trajectory\n",
    "#==============================================================================\n",
    "\n",
    "# Start training on each trajectory separately\n",
    "for idx, train_loader in enumerate(train_loaders):\n",
    "    print(f'Training on trajectory {idx + 1}')\n",
    "    model, optimizer, epoch_hist = train(model, train_loader, lr, wd, lamb, epochs,\n",
    "                                         lr_decay, lr_update, nu, eta, backward,\n",
    "                                         steps, steps_back, gradclip)\n",
    "    # Save the trained model after training on each trajectory\n",
    "    torch.save(model.state_dict(), f'model_trained_on_trajectory_{idx + 1}.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
